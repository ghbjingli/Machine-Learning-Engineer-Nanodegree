{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(8)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test_cm = pd.read_csv('test.csv')\n",
    "test_lb = pd.read_csv('test_labels.csv')\n",
    "test_all = pd.merge(test_cm, test_lb, on='id')\n",
    "test = test_all[test_all['toxic'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      "id               159571 non-null object\n",
      "comment_text     159571 non-null object\n",
      "toxic            159571 non-null int64\n",
      "severe_toxic     159571 non-null int64\n",
      "obscene          159571 non-null int64\n",
      "threat           159571 non-null int64\n",
      "insult           159571 non-null int64\n",
      "identity_hate    159571 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cm = train[\"comment_text\"].values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "test_cm = test[\"comment_text\"].values\n",
    "y_test = test[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 100000\n",
    "maxlen = 226\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    " \n",
    "t = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "X_train_ = []\n",
    "#for i, v in train_cm.iteritems():\n",
    "for v in train_cm:\n",
    "    words = t.tokenize(v)\n",
    "    words_filtered = [word for word in words if word not in stop_words]\n",
    "    X_train_.append(\" \".join(words_filtered))\n",
    "\n",
    "X_test_ = []\n",
    "for v in test_cm:\n",
    "    words = t.tokenize(v)\n",
    "    words_filtered = [word for word in words if word not in stop_words]\n",
    "    X_test_.append(\" \".join(words_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = text.Tokenizer(num_words=vocab)\n",
    "t.fit_on_texts(list(X_train_) + list(X_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = t.texts_to_sequences(X_train_)\n",
    "X_test = t.texts_to_sequences(X_test_)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explanation Why edits made username Hardcore Metallica Fan reverted They vandalisms closure GAs I voted New York Dolls FAC And please remove template talk page since I retired 89 205 38 27'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[601,\n",
       " 165,\n",
       " 60,\n",
       " 57,\n",
       " 593,\n",
       " 4558,\n",
       " 10985,\n",
       " 1097,\n",
       " 262,\n",
       " 211,\n",
       " 13059,\n",
       " 6657,\n",
       " 2589,\n",
       " 1,\n",
       " 2831,\n",
       " 45,\n",
       " 1139,\n",
       " 16501,\n",
       " 2456,\n",
       " 35,\n",
       " 10,\n",
       " 167,\n",
       " 274,\n",
       " 7,\n",
       " 5,\n",
       " 66,\n",
       " 1,\n",
       " 3510,\n",
       " 3531,\n",
       " 5422,\n",
       " 2383,\n",
       " 1012]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 pre-trained words\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = 'crawl-300d-2M.vec'\n",
    "embeddings_index = dict()\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s pre-trained words' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab, embed_size))\n",
    "for word, i in t.word_index.items():\n",
    "    if i >= vocab: \n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_model(activation, filters, Sdroprate, droprate, layers):\n",
    "\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(vocab, embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False)(inp)\n",
    "    x = SpatialDropout1D(Sdroprate)(x)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "\n",
    "    conv_0 = Conv2D(filters, kernel_size=(2, embed_size), activation=activation)(x)\n",
    "    maxpool_0 = MaxPool2D(pool_size=(maxlen - 1, 1))(conv_0)\n",
    "    if layers == 2:   \n",
    "        conv_1 = Conv2D(filters, kernel_size=(3, embed_size), activation=activation)(x)\n",
    "        maxpool_1 = MaxPool2D(pool_size=(maxlen - 2, 1))(conv_1)\n",
    "        y = Concatenate(axis=1)([maxpool_0, maxpool_1])\n",
    "    elif layers == 3:\n",
    "        conv_1 = Conv2D(filters, kernel_size=(3, embed_size), activation=activation)(x)\n",
    "        maxpool_1 = MaxPool2D(pool_size=(maxlen - 2, 1))(conv_1)\n",
    "        conv_2 = Conv2D(filters, kernel_size=(4, embed_size), activation=activation)(x)\n",
    "        maxpool_2 = MaxPool2D(pool_size=(maxlen - 3, 1))(conv_2) \n",
    "        y = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "    elif layers == 4:\n",
    "        conv_1 = Conv2D(filters, kernel_size=(3, embed_size), activation=activation)(x)\n",
    "        maxpool_1 = MaxPool2D(pool_size=(maxlen - 2, 1))(conv_1)\n",
    "        conv_2 = Conv2D(filters, kernel_size=(4, embed_size), activation=activation)(x)\n",
    "        maxpool_2 = MaxPool2D(pool_size=(maxlen - 3, 1))(conv_2)\n",
    "        conv_3 = Conv2D(filters, kernel_size=(5, embed_size), activation=activation)(x)\n",
    "        maxpool_3 = MaxPool2D(pool_size=(maxlen - 4, 1))(conv_3)\n",
    "        y = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n",
    "    elif layers == 5:\n",
    "        conv_1 = Conv2D(filters, kernel_size=(3, embed_size), activation=activation)(x)\n",
    "        maxpool_1 = MaxPool2D(pool_size=(maxlen - 2, 1))(conv_1)\n",
    "        conv_2 = Conv2D(filters, kernel_size=(4, embed_size), activation=activation)(x)\n",
    "        maxpool_2 = MaxPool2D(pool_size=(maxlen - 3, 1))(conv_2)\n",
    "        conv_3 = Conv2D(filters, kernel_size=(5, embed_size), activation=activation)(x)\n",
    "        maxpool_3 = MaxPool2D(pool_size=(maxlen - 4, 1))(conv_3)\n",
    "        conv_4 = Conv2D(filters, kernel_size=(6, embed_size), activation=activation)(x)\n",
    "        maxpool_4 = MaxPool2D(pool_size=(maxlen - 5, 1))(conv_4)\n",
    "        y = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3, maxpool_4])\n",
    "    else:\n",
    "        conv_1 = Conv2D(filters, kernel_size=(3, embed_size), activation=activation)(x)\n",
    "        maxpool_1 = MaxPool2D(pool_size=(maxlen - 2, 1))(conv_1)\n",
    "        conv_2 = Conv2D(filters, kernel_size=(4, embed_size), activation=activation)(x)\n",
    "        maxpool_2 = MaxPool2D(pool_size=(maxlen - 3, 1))(conv_2)\n",
    "        conv_3 = Conv2D(filters, kernel_size=(5, embed_size), activation=activation)(x)\n",
    "        maxpool_3 = MaxPool2D(pool_size=(maxlen - 4, 1))(conv_3)\n",
    "        conv_4 = Conv2D(filters, kernel_size=(6, embed_size), activation=activation)(x)\n",
    "        maxpool_4 = MaxPool2D(pool_size=(maxlen - 5, 1))(conv_4)\n",
    "        conv_5 = Conv2D(filters, kernel_size=(7, embed_size), activation=activation)(x)\n",
    "        maxpool_5 = MaxPool2D(pool_size=(maxlen - 6, 1))(conv_5)\n",
    "        y = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3, maxpool_4, maxpool_5])\n",
    "    \n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(droprate)(y)\n",
    "\n",
    "    outp = Dense(6, activation=\"sigmoid\")(y)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "#model = make_model()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits = 1, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "my_classifier = KerasClassifier(make_model, batch_size=256)\n",
    "\n",
    "validator = GridSearchCV(my_classifier, param_grid={'activation': ['tanh'],\n",
    "                                                   'epochs': [5],\n",
    "                                                   'filters': [32],\n",
    "                                                   'Sdroprate': [0.4],\n",
    "                                                   'droprate': [0.1],\n",
    "                                                   'layers': [4]},\n",
    "                                                   cv = cv, \n",
    "                                                   scoring = 'roc_auc', \n",
    "                                                   n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "143613/143613 [==============================] - 525s 4ms/step - loss: 0.0821 - acc: 0.9745\n",
      "Epoch 2/5\n",
      "143613/143613 [==============================] - 529s 4ms/step - loss: 0.0509 - acc: 0.9811\n",
      "Epoch 3/5\n",
      "143613/143613 [==============================] - 530s 4ms/step - loss: 0.0472 - acc: 0.9821\n",
      "Epoch 4/5\n",
      "143613/143613 [==============================] - 530s 4ms/step - loss: 0.0449 - acc: 0.9827\n",
      "Epoch 5/5\n",
      "143613/143613 [==============================] - 531s 4ms/step - loss: 0.0438 - acc: 0.9830\n",
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 599s 4ms/step - loss: 0.0801 - acc: 0.9747\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 614s 4ms/step - loss: 0.0502 - acc: 0.9813\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 602s 4ms/step - loss: 0.0469 - acc: 0.9821\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 591s 4ms/step - loss: 0.0449 - acc: 0.9827\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 588s 4ms/step - loss: 0.0435 - acc: 0.9831\n"
     ]
    }
   ],
   "source": [
    "grid_result = validator.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.988373 with {'Sdroprate': 0.4, 'activation': 'tanh', 'droprate': 0.1, 'epochs': 5, 'filters': 32, 'layers': 4}\n",
      "0.988373 (0.000000) with: {'Sdroprate': 0.4, 'activation': 'tanh', 'droprate': 0.1, 'epochs': 5, 'filters': 32, 'layers': 4}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %f with %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# validator.best_estimator_.model returns the (unwrapped) keras model\n",
    "best_model = validator.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(x_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " roc_auc score for keras model: 0.982279 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = roc_auc_score(y_test, y_pred)\n",
    "print(\"\\n roc_auc score for keras model: %.6f \\n\" % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x1a5722fdd8>\n"
     ]
    }
   ],
   "source": [
    "print(grid_result.best_estimator_.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_scorer(roc_auc_score, needs_threshold=True)\n"
     ]
    }
   ],
   "source": [
    "print(grid_result.scorer_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:toxic2]",
   "language": "python",
   "name": "conda-env-toxic2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
