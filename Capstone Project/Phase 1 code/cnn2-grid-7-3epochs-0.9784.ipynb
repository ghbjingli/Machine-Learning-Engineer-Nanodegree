{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(8)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read in train and test datasets from csv files\n",
    "train = pd.read_csv('train.csv')\n",
    "test_cm = pd.read_csv('test.csv')\n",
    "test_lb = pd.read_csv('test_labels.csv')\n",
    "# Merge test comments with test labels\n",
    "test_all = pd.merge(test_cm, test_lb, on='id')\n",
    "# Remove all test entries with labels equal to -1\n",
    "test = test_all[test_all['toxic'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"comment_text\"]\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"]\n",
    "y_test = test[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary size\n",
    "vocab = 100000\n",
    "# Define maximum length of a comment\n",
    "maxlen = 200\n",
    "# Define embedding size which should equal the embedding size of the pre-trained word vectors\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ip address\n",
    "X_train = X_train.apply(lambda x: re.sub(\"(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\",\"\",x))\n",
    "X_test = X_test.apply(lambda x: re.sub(\"(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\",\"\",x))\n",
    "# Removing url link\n",
    "X_train = X_train.apply(lambda x: re.sub(\"http://.*com\",\"\",x))\n",
    "X_test = X_test.apply(lambda x: re.sub(\"http://.*com\",\"\",x))\n",
    "# Removing username\n",
    "X_train = X_train.apply(lambda x: re.sub(\"\\[\\[.*\\]\",\"\",x))\n",
    "X_test = X_test.apply(lambda x: re.sub(\"\\[\\[.*\\]\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def filter_stop_words(sentences, stop_words):\n",
    "    filtered = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        words_filtered = [word for word in words if word not in stop_words]\n",
    "        filtered.append(\" \".join(words_filtered))\n",
    "    return filtered\n",
    " \n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    " \n",
    "# Comments in train\n",
    "X_train_ = filter_stop_words(X_train, stop_words)\n",
    " \n",
    "# Comments in test (excluding labels with -1 values)\n",
    "X_test_ = filter_stop_words(X_test, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the train dataset\n",
    "t = text.Tokenizer(num_words=vocab)\n",
    "t.fit_on_texts(list(X_train_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both train and test datasets into sequences\n",
    "X_train = t.texts_to_sequences(X_train_)\n",
    "X_test = t.texts_to_sequences(X_test_)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 pre-trained words\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained word vectors\n",
    "EMBEDDING_FILE = 'crawl-300d-2M.vec'\n",
    "embeddings_index = dict()\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s pre-trained words' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab, embed_size))\n",
    "for word, i in t.word_index.items():\n",
    "    if i >= vocab: \n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 200, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 194, 1, 100)       210100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 30,210,706\n",
      "Trainable params: 210,706\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define make_model function to create a CNN model\n",
    "def make_model(k=[7], activation='relu', filters=100, Sdroprate=0.5, droprate=0.0):\n",
    "\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(vocab, embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False)(inp)\n",
    "    x = SpatialDropout1D(Sdroprate)(x)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "    \n",
    "    conv = dict()\n",
    "    maxpool = dict()\n",
    "    for h in k:\n",
    "        conv[h] = Conv2D(filters, kernel_size=(h, embed_size), activation=activation)(x)\n",
    "        maxpool[h] = MaxPool2D(pool_size=(maxlen - h + 1, 1))(conv[h])\n",
    "        \n",
    "    if len(k) == 1:\n",
    "        y = maxpool[h]\n",
    "    else:\n",
    "        y = Concatenate(axis=1)([pool for key,pool in maxpool.items()])\n",
    "    \n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(droprate)(y)\n",
    "\n",
    "    outp = Dense(6, activation=\"sigmoid\")(y)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Generate the model based on the default parameters\n",
    "model = make_model()\n",
    "# Print out the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross validation split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits = 2, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap Keras model with KerasClassifier so that it can be used in Sklearn GridSearchCV\n",
    "# Generate a GridSearchCV instance with the parameters to be tuned\n",
    "# This is a starting model and we try to check which activation function performs better\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "my_classifier = KerasClassifier(make_model, verbose=1)\n",
    "\n",
    "validator = GridSearchCV(my_classifier, param_grid={'k': [[7]],\n",
    "                                                    'activation': ['relu'],\n",
    "                                                   'filters': [100],\n",
    "                                                   'Sdroprate':[0.3,0.4,0.5],\n",
    "                                                   'droprate': [0.00],\n",
    "                                                   'epochs': [3],\n",
    "                                                   'batch_size': [128]},\n",
    "                                                    cv = cv, \n",
    "                                                    scoring = 'roc_auc', \n",
    "                                                    verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] Sdroprate=0.3, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 681s 5ms/step - loss: 0.0645 - acc: 0.9791\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 670s 5ms/step - loss: 0.0460 - acc: 0.9828\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 666s 5ms/step - loss: 0.0413 - acc: 0.9843\n",
      "15958/15958 [==============================] - 34s 2ms/step\n",
      "143613/143613 [==============================] - 304s 2ms/step\n",
      "[CV]  Sdroprate=0.3, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7], score=0.9882000380246808, total=34.2min\n",
      "[CV] Sdroprate=0.3, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 39.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 664s 5ms/step - loss: 0.0670 - acc: 0.9779\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 663s 5ms/step - loss: 0.0458 - acc: 0.9827\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 664s 5ms/step - loss: 0.0412 - acc: 0.9842\n",
      "15958/15958 [==============================] - 34s 2ms/step\n",
      "143613/143613 [==============================] - 303s 2ms/step\n",
      "[CV]  Sdroprate=0.3, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7], score=0.984340767209594, total=33.8min\n",
      "[CV] Sdroprate=0.4, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 78.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 662s 5ms/step - loss: 0.0672 - acc: 0.9781\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 661s 5ms/step - loss: 0.0476 - acc: 0.9822\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 661s 5ms/step - loss: 0.0437 - acc: 0.9835\n",
      "15958/15958 [==============================] - 34s 2ms/step\n",
      "143613/143613 [==============================] - 303s 2ms/step\n",
      "[CV]  Sdroprate=0.4, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7], score=0.9877251870938318, total=33.7min\n",
      "[CV] Sdroprate=0.4, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 116.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 662s 5ms/step - loss: 0.0650 - acc: 0.9789\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 661s 5ms/step - loss: 0.0473 - acc: 0.9822\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 660s 5ms/step - loss: 0.0435 - acc: 0.9834\n",
      "15958/15958 [==============================] - 34s 2ms/step\n",
      "143613/143613 [==============================] - 303s 2ms/step\n",
      "[CV]  Sdroprate=0.4, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7], score=0.984133161808232, total=33.6min\n",
      "[CV] Sdroprate=0.5, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 155.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 661s 5ms/step - loss: 0.0681 - acc: 0.9780\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 660s 5ms/step - loss: 0.0500 - acc: 0.9816\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 661s 5ms/step - loss: 0.0465 - acc: 0.9826\n",
      "15958/15958 [==============================] - 34s 2ms/step\n",
      "143613/143613 [==============================] - 303s 2ms/step\n",
      "[CV]  Sdroprate=0.5, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7], score=0.9876272808634594, total=33.6min\n",
      "[CV] Sdroprate=0.5, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 194.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 662s 5ms/step - loss: 0.0699 - acc: 0.9775\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 661s 5ms/step - loss: 0.0496 - acc: 0.9816\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 661s 5ms/step - loss: 0.0462 - acc: 0.9826\n",
      "15958/15958 [==============================] - 34s 2ms/step\n",
      "143613/143613 [==============================] - 303s 2ms/step\n",
      "[CV]  Sdroprate=0.5, activation=relu, batch_size=128, droprate=0.0, epochs=3, filters=100, k=[7], score=0.9826468991886611, total=33.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 232.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 232.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "159571/159571 [==============================] - 748s 5ms/step - loss: 0.0637 - acc: 0.9789\n",
      "Epoch 2/3\n",
      "159571/159571 [==============================] - 738s 5ms/step - loss: 0.0456 - acc: 0.9828\n",
      "Epoch 3/3\n",
      "159571/159571 [==============================] - 737s 5ms/step - loss: 0.0409 - acc: 0.9843\n"
     ]
    }
   ],
   "source": [
    "# Run the Grid Search\n",
    "grid_result = validator.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.986270 with {'Sdroprate': 0.3, 'activation': 'relu', 'batch_size': 128, 'droprate': 0.0, 'epochs': 3, 'filters': 100, 'k': [7]}\n",
      "0.986270 (0.001930) with: {'Sdroprate': 0.3, 'activation': 'relu', 'batch_size': 128, 'droprate': 0.0, 'epochs': 3, 'filters': 100, 'k': [7]}\n",
      "0.985929 (0.001796) with: {'Sdroprate': 0.4, 'activation': 'relu', 'batch_size': 128, 'droprate': 0.0, 'epochs': 3, 'filters': 100, 'k': [7]}\n",
      "0.985137 (0.002490) with: {'Sdroprate': 0.5, 'activation': 'relu', 'batch_size': 128, 'droprate': 0.0, 'epochs': 3, 'filters': 100, 'k': [7]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %f with %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# Get the best model\n",
    "best_model = validator.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction with unseen test dataset with the best model\n",
    "y_pred = best_model.predict(x_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " roc_auc score for keras model: 0.978442 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the roc_auc score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "print(\"\\n roc_auc score for keras model: %.6f \\n\" % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 200, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_8 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 200, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 194, 1, 100)       210100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 30,210,706\n",
      "Trainable params: 210,706\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generate a summary report for the best model architecture\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:toxic3]",
   "language": "python",
   "name": "conda-env-toxic3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
